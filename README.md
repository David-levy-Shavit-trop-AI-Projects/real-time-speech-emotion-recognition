# Real-Time Speech Emotion Recognition (SER)

**Authors:** [Shavit Trop](https://linkedin.com/in/shavit-trop) & [David Levy](https://www.linkedin.com/in/dudi-levy)  

This project implements a **real-time Speech Emotion Recognition (SER)** with GUI system as part of an **BSc in Computer Science â€“ Deep Learning course final project**.

The focus of the project is **engineering a low-latency, real-time pipeline** rather than pushing state-of-the-art research accuracy. The system is built using **PyTorch** and performs **live emotion classification from microphone input**.

---

## ðŸŽ¯ Project Goals

- Train a deep learning model to recognize emotions from speech
- Perform **real-time inference** from a microphone stream
- Emphasize **engineering quality**, modularity, and reproducibility
- Separate **training** and **deployment (GUI)** workflows

---

## ðŸ“Š Dataset: [RAVDESS](https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)

**Key properties:**
- 1440 files
- 24 Professional actors (12 male & 12 female)
- Clean studio-quality recordings
- Speech-only subset used
- 8 emotion classes:
  - Neutral
  - Calm
  - Happy
  - Sad
  - Angry
  - Fearful
  - Disgust
  - Surprised

The dataset is downloaded automatically using the Kaggle API:

```python
import kagglehub
path = kagglehub.dataset_download("uwrfkaggler/ravdess-emotional-speech-audio")
```

---

## ðŸ§  Model Architecture

The model is designed for **real-time performance**:

- **Log-Mel Spectrogram** input
- **CNN feature extractor** (spatial patterns)
- **GRU temporal model** (short-term dynamics)
- Lightweight architecture for low-latency inference

All audio, feature, and model hyperparameters are centralized in `config.py` to ensure consistency between training and inference.

---

## ðŸ—‚ Project Structure

```
project/
â”‚
â”œâ”€â”€ cnn_gru_ravdess.pth   # Trained model (ignored by git)
â”œâ”€â”€ config.py             # Shared configuration
â”œâ”€â”€ realtime_gui.py       # Live microphone GUI
â”œâ”€â”€ requirements.txt      # dependancies list
â”œâ”€â”€ training_model.ipynb  # Training, evaluation, plots
â””â”€â”€ README.md
```

---

## ðŸš€ How to Run the Project

### prerequisites

- Python 3

---

### 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```
---

### 2ï¸âƒ£ Train the Model (First time users)

Open and "Run All":

```
training_model.ipynb
```

This notebook will:
- Downloads the RAVDESS dataset
- Extracts log-mel features
- Trains the CNN-GRU model
- Plots:
  - Training & validation loss/accuracy
  - Confusion matrix
  - Classification report
- Saves the trained model to:

```
cnn_gru_ravdess.pth
```

---

### 3ï¸âƒ£ Run the Real-Time GUI

Only after the training is complete (at least once), run the following command on terminal:

```
python realtime_gui.py
```

This command will:
- Loads the trained model
- Captures live audio from the microphone
- Performs streaming inference
- Displays real-time emotion probabilities

> You **do not need to retrain** the model to run the GUI.

---

## ðŸ§ª Engineering Highlights

- Centralized configuration (`config.py`)
- Trainâ€“inference parameter consistency
- Modular notebooks for faster demos
- Real-time sliding window inference
- Clean separation of concerns

---

## ðŸŽ“ Academic Context

This project was developed as a **final project** in a graduate-level **Deep Learning** course, with emphasis on:

- Practical deep learning systems
- Reproducible experimentation
- Real-time AI deployment considerations

---

## ðŸ“Œ Notes

- Trained model weights are excluded from version control (`.gitignore`)
- For reproducibility, retrain the model locally
- Accuracy is constrained by real-time requirements and dataset size

---

## ðŸ“¬ Future Improvements

- Voice Activity Detection (VAD)
- Model quantization / TorchScript export
- Latency and FPS monitoring
- Noise-robust feature extraction
